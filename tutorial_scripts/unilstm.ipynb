{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "potpourri-unilstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "077fe79fc7de451c98685fd8e35f8f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9fd9549174844409c1d4e01bf6c97ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3a2667bfe3754275b65ebb6d0b1f3eb2",
              "IPY_MODEL_fb41f9d75d6c4f8ea57368d61f6c83d7"
            ]
          }
        },
        "e9fd9549174844409c1d4e01bf6c97ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "3a2667bfe3754275b65ebb6d0b1f3eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aec5b17d66ab4028987ddd411fffabed",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2b68b2c2088497e8951731834d2dd21"
          }
        },
        "fb41f9d75d6c4f8ea57368d61f6c83d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6c512db085124c36a4704f14deeb4fc1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 25/25 [00:00&lt;00:00, 126.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4bfb91e81fa9425580c2590ad964cac4"
          }
        },
        "aec5b17d66ab4028987ddd411fffabed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2b68b2c2088497e8951731834d2dd21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c512db085124c36a4704f14deeb4fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4bfb91e81fa9425580c2590ad964cac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6BtzCixafQN"
      },
      "source": [
        "This script:  \n",
        "1. Loads GloVe Twitter embeddings\n",
        "2. Loads an example arbitrary dataset in a Pandas DataFrame\n",
        "3. Initializes a PyTorch uni-directional LSTM model\n",
        "4. Wraps this model in a [PyTorch Lightning](https://www.pytorchlightning.ai/)'s [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html)\n",
        "5. Trains this model\n",
        "\n",
        "NOTE: this script is still a [WIP](https://www.urbandictionary.com/define.php?term=Wip)! PRs are welcome.  \n",
        "\n",
        "Future plans:  \n",
        "1. Incorporate hyperparameter tuning using [Ray Tune](https://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html).\n",
        "2. Incorporate EarlyStopping using [PyTorch Lightning callbacks](https://pytorch-lightning.readthedocs.io/en/1.2.0/extensions/generated/pytorch_lightning.callbacks.EarlyStopping.html?highlight=earlystopping).\n",
        "3. Better commenting, including that of the model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjXA0s4lq04C"
      },
      "source": [
        "Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfAeszCLdt3i"
      },
      "source": [
        "!pip install pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVQm62sCdho8"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF9XwY2WfQc8"
      },
      "source": [
        "Download GloVe embeddings.  \n",
        "\n",
        "I have already downloaded the [glove.twitter.27B.200d](https://nlp.stanford.edu/projects/glove/) embeddings and converted them into a convenient format:  \n",
        "1. `terms.npy` (~25MB): a numpy array of shape (size of vocabulary,), each element a *token*.  \n",
        "2. `embs.npy` (~2GB): a numpy array of shape (size of vocabulary, embeddings dimension), each row an embedding for corresponding token.  \n",
        "\n",
        "Feel free to:\n",
        "1. pre-process and load your own embeddings to pass to the model using a similar format (vocabulary in `terms.npy` and the actual embeddings in `embs.npy`).  \n",
        "2. randomly initialize embeddings which can then be trained along with the rest of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "VVzj5jgAPmMD",
        "outputId": "fb357f4f-be2b-499b-91ae-bed9d57b05f1"
      },
      "source": [
        "import gdown\n",
        "glove_terms_url = 'https://drive.google.com/uc?id=1-3B9Zq9DJphA4TdtuxigNfQNrdUhZlhn'\n",
        "gdown.download(glove_terms_url, 'glove.twitter.27B.200d.terms.npy', quiet=False)\n",
        "\n",
        "glove_embs_url = 'https://drive.google.com/uc?id=1-A7WSkxmaRS27BmBGtCfWizk_sv37QDY'\n",
        "gdown.download(glove_embs_url, 'glove.twitter.27B.200d.embs.npy', quiet=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-3B9Zq9DJphA4TdtuxigNfQNrdUhZlhn\n",
            "To: /content/glove.twitter.27B.200d.terms.npy\n",
            "24.4MB [00:00, 130MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-A7WSkxmaRS27BmBGtCfWizk_sv37QDY\n",
            "To: /content/glove.twitter.27B.200d.embs.npy\n",
            "1.91GB [00:16, 118MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'glove.twitter.27B.200d.embs.npy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4xsIvcfdjC1"
      },
      "source": [
        "Load the downloaded embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjnLu9nDgN5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3aea6f-3ed2-4244-afe9-988b3fcc0fb7"
      },
      "source": [
        "glove_terms_npa = np.load('glove.twitter.27B.200d.terms.npy',allow_pickle=True)\n",
        "PAD_TOK, UNK_TOK = '<pad>','<unk>'\n",
        "glove_terms_npa = np.concatenate([[PAD_TOK, UNK_TOK],glove_terms_npa])\n",
        "\n",
        "glove_embs_npa = np.load('glove.twitter.27B.200d.embs.npy',allow_pickle=True)\n",
        "glove_embs_npa = np.concatenate([np.zeros((1,glove_embs_npa.shape[1])),np.mean(glove_embs_npa,axis=0,keepdims=1),glove_embs_npa],axis=0)\n",
        "\n",
        "print(glove_terms_npa.shape)\n",
        "print(glove_embs_npa.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1193516,)\n",
            "(1193516, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxvHotWWDqhR"
      },
      "source": [
        "Load dataset  \n",
        "\n",
        "In this script, we work with the task of **irony detection**.  \n",
        "**Feel free to load your own dataset instead.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1jvSVMGmocm"
      },
      "source": [
        "def lower_text(row):\n",
        "    row.text = row.text.lower()\n",
        "    return row\n",
        "def preprocess_data_df(df):\n",
        "    df = df.apply(lower_text,axis=1)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WH4cpn50CqQ"
      },
      "source": [
        "!test -d /tmp/tweeteval && echo \"FYI: tweeteval directory already exists, to pull latest version uncomment this line below: !rm -r tweeteval\"\n",
        "#!rm -r /tmp/tweeteval\n",
        "!test -d /tmp/tweeteval || git clone https://github.com/cardiffnlp/tweeteval /tmp/tweeteval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwv-abuVF3R6"
      },
      "source": [
        "#####Load train data#####\n",
        "with open('/tmp/tweeteval/datasets/irony/train_text.txt','rt') as fi:\n",
        "    train_texts = fi.read().strip().split('\\n')\n",
        "train_text_dfs = pd.Series(data=train_texts,name='text',dtype='str')\n",
        "train_labels_dfs = pd.read_csv('/tmp/tweeteval/datasets/irony/train_labels.txt',names=['label'],index_col=False).label\n",
        "irony_train_df = pd.concat([train_text_dfs,train_labels_dfs],axis=1)\n",
        "#####Load val data#######\n",
        "with open('/tmp/tweeteval/datasets/irony/val_text.txt','rt') as fi:\n",
        "    val_texts = fi.read().strip().split('\\n')\n",
        "val_text_dfs = pd.Series(data=val_texts,name='text',dtype='str')\n",
        "val_labels_dfs = pd.read_csv('/tmp/tweeteval/datasets/irony/val_labels.txt',names=['label'],index_col=False).label\n",
        "irony_val_df = pd.concat([val_text_dfs,val_labels_dfs],axis=1)\n",
        "#####Load test data######\n",
        "with open('/tmp/tweeteval/datasets/irony/test_text.txt','rt') as fi:\n",
        "    test_texts = fi.read().strip().split('\\n')\n",
        "test_text_dfs = pd.Series(data=test_texts,name='text',dtype='str')\n",
        "test_labels_dfs = pd.read_csv('/tmp/tweeteval/datasets/irony/test_labels.txt',names=['label'],index_col=False).label\n",
        "irony_test_df = pd.concat([test_text_dfs,test_labels_dfs],axis=1)\n",
        "#######Preprocess########\n",
        "irony_train_df = preprocess_data_df(irony_train_df)\n",
        "irony_val_df = preprocess_data_df(irony_val_df)\n",
        "irony_test_df = preprocess_data_df(irony_test_df)\n",
        "#######Clean up##########\n",
        "train_text_dfs,train_labels_dfs = None,None\n",
        "val_text_dfs,val_labels_dfs = None,None\n",
        "test_text_dfs,test_labels_dfs = None,None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLKtcSSFfWi4"
      },
      "source": [
        "Design the [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) which can then later be loaded into a [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).  \n",
        "We are creating a map-style Dataset as the example data is already loaded in the memory. Each Dataset must overwrite the `__getitem__()` method. Each call to this method must fetch the `sample_id`th sample from the underlying data.  \n",
        "\n",
        "\\[In case you have a streamed dataset (eg- data streaming-in live from a file descriptor), consider using an [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset) instead.\\]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNPbGMzufIIX"
      },
      "source": [
        "class LSTMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, vocab, max_seq_length, pad_token, unk_token):\n",
        "        self.labels = df.label.tolist()\n",
        "        self.word2idx = {term:idx for idx,term in enumerate(vocab)}\n",
        "        self.idx2word = {idx:word for word,idx in self.word2idx.items()}\n",
        "        \n",
        "        self.pad_token,self.unk_token = pad_token,unk_token\n",
        "        self.input_ids = []\n",
        "        self.sequence_lens = []\n",
        "        self.labels = []\n",
        "        for i in range(df.shape[0]):\n",
        "            input_ids,sequence_len = self.convert_text_to_input_ids(df.iloc[i].text,pad_to_len=max_seq_length)\n",
        "            \n",
        "            self.input_ids.append(input_ids.reshape(-1))\n",
        "            self.sequence_lens.append(sequence_len)\n",
        "            self.labels.append(df.iloc[i].label)\n",
        "        \n",
        "        assert len(self.input_ids) == df.shape[0]\n",
        "        assert len(self.sequence_lens) == df.shape[0]\n",
        "        assert len(self.labels) == df.shape[0]\n",
        "    \n",
        "    def convert_text_to_input_ids(self,text,pad_to_len):\n",
        "        words = text.strip().split()[:pad_to_len]\n",
        "        deficit = pad_to_len - len(words)\n",
        "        words.extend([self.pad_token]*deficit)\n",
        "        for i in range(len(words)):\n",
        "            if words[i] not in self.word2idx:\n",
        "                words[i] = self.word2idx[self.unk_token]\n",
        "            else:\n",
        "                words[i] = self.word2idx[words[i]]\n",
        "        return torch.Tensor(words).long(),pad_to_len - deficit\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self, sample_id):\n",
        "        sample_dict = dict()\n",
        "        sample_dict['input_ids'] = self.input_ids[sample_id].reshape(-1)\n",
        "        sample_dict['sequence_len'] = torch.tensor(self.sequence_lens[sample_id]).long()\n",
        "        sample_dict['labels'] = torch.tensor(self.labels[sample_id])\n",
        "        return sample_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHCCrydUEfQ8"
      },
      "source": [
        "Design the LSTM encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbi_NEuNbDqH"
      },
      "source": [
        "class LSTMEncoder(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        \n",
        "        pretrained_embeddings = config['pretrained_embeddings'] if 'pretrained_embeddings' in config else None\n",
        "        freeze_embeddings = config['freeze_embeddings'] if 'freeze_embeddings' in config else False\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.vocab_size = pretrained_embeddings.shape[0]\n",
        "            self.embedding_dim = pretrained_embeddings.shape[1]\n",
        "            self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(pretrained_embeddings).float(),freeze=freeze_embeddings)\n",
        "        else:\n",
        "            assert 'vocab' in config and 'embedding_dim' in config\n",
        "            self.vocab_size = config['vocab'].shape[0]\n",
        "            self.embedding_dim = config['embedding_dim']\n",
        "            if freeze_embeddings:\n",
        "                print('warning: freezing randomly initialized embeddings')\n",
        "            self.embedding = torch.nn.Embedding(self.vocab_size,self.embedding_dim,freeze=freeze_embeddings)\n",
        "        \n",
        "        self.hidden_size = config['hidden_size']\n",
        "        lstm_unit_cnt = config['lstm_unit_cnt']\n",
        "        self.lstm = torch.nn.LSTM(input_size=self.embedding_dim,hidden_size=self.hidden_size,num_layers=lstm_unit_cnt,batch_first=True,bidirectional=False)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        x = batch['input_ids']\n",
        "        x_lengths = batch['sequence_len']\n",
        "        embed_out = self.embedding(x)\n",
        "        packed_input = torch.nn.utils.rnn.pack_padded_sequence(embed_out, x_lengths.tolist(),enforce_sorted=False,batch_first=True)\n",
        "        packed_out,_ = self.lstm(packed_input)\n",
        "        output,_ = torch.nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)    #inverse operation of pack_padded_sequence\n",
        "        lstm_out = output[range(len(output)), x_lengths - 1, :self.hidden_size]\n",
        "        return lstm_out\n",
        "    \n",
        "    def get_embedding_dims(self):\n",
        "        return self.vocab_size,self.embedding_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v01KgLRFEmuo"
      },
      "source": [
        "Design the Pytorch Lightning's [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrEfWjZC7OAq"
      },
      "source": [
        "class LitClassifier(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        #Embeddings init\n",
        "        self.vocab = config['vocab']\n",
        "        self.vocab_size = self.vocab.shape[0]\n",
        "        self.pad_token = config['pad_token']\n",
        "        self.unk_token = config['unk_token']\n",
        "        #Model init\n",
        "        self.label_count = config['label_count']\n",
        "        self.model = LSTMEncoder(config)\n",
        "        self.lr = config['lr']\n",
        "        model_out_size = self.model.hidden_size\n",
        "        self.pooler_layer = torch.nn.Linear(model_out_size, model_out_size)\n",
        "        self.dropout = torch.nn.Dropout(config['dropout_prob'])\n",
        "        self.classifier = torch.nn.Linear(model_out_size, self.label_count)\n",
        "\n",
        "        #Data init\n",
        "        if 'df' in config:\n",
        "            self.df = config['df']\n",
        "            self.train_df,self.val_df,self.test_df = None,None,None\n",
        "        elif 'train_df' in config and 'val_df' in config and 'test_df' in config:\n",
        "            self.train_df = config['train_df'] if 'val_df' in config else None\n",
        "            self.val_df = config['val_df'] if 'val_df' in config else None\n",
        "            self.test_df = config['test_df'] if 'test_df' in config else None\n",
        "            self.df = None\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        self.batch_size = config['batch_size']# if 'batch_size' in config else 32\n",
        "        self.max_seq_length = config['max_seq_length']\n",
        "        self.labels = self.train_df.label.tolist()\n",
        "        \n",
        "        #self.criterion = torch.nn.CrossEntropyLoss(ignore_index = self.TRG.vocab.stoi[self.TRG.pad_token])\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    def shared_step(self, batch, batch_idx):\n",
        "\n",
        "        batched_contextual_embeddings = self.model(batch)\n",
        "        \n",
        "        pooler_layer_out = self.pooler_layer(batched_contextual_embeddings)\n",
        "\n",
        "        pooler_layer_out = self.dropout(pooler_layer_out)\n",
        "        logits = self.classifier(pooler_layer_out)\n",
        "        pred_labels = torch.argmax(logits,dim=1)\n",
        "        actual_labels = batch['labels']\n",
        "        assert actual_labels.shape[0] == pred_labels.shape[0]\n",
        "\n",
        "        loss = self.criterion(logits, actual_labels)\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        pred_labels = pred_labels.detach().cpu().numpy()\n",
        "        actual_labels = actual_labels.detach().cpu().numpy()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        metrics['loss'] = loss\n",
        "        metrics['acc'] = (pred_labels == actual_labels).sum() / pred_labels.shape[0]\n",
        "        metrics['macro_f1'] = f1_score(actual_labels,pred_labels,average='macro')\n",
        "\n",
        "        return_dict = {'loss':loss,'metrics':metrics}\n",
        "\n",
        "        return return_dict\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        shared_step_out_dict = self.shared_step(batch, batch_idx)\n",
        "        loss,metrics = shared_step_out_dict['loss'],shared_step_out_dict['metrics']\n",
        "\n",
        "        metrics = {'train_'+k:v for k,v in metrics.items()}\n",
        "        self.log_dict(metrics,on_epoch=True,prog_bar=True,logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        shared_step_out_dict = self.shared_step(batch, batch_idx)\n",
        "        loss,metrics = shared_step_out_dict['loss'],shared_step_out_dict['metrics']\n",
        "\n",
        "        metrics = {'val_'+k:v for k,v in metrics.items()}\n",
        "        self.log_dict(metrics,on_epoch=True,prog_bar=True,logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        shared_step_out_dict = self.shared_step(batch, batch_idx)\n",
        "        loss,metrics = shared_step_out_dict['loss'],shared_step_out_dict['metrics']\n",
        "\n",
        "        metrics = {'test_'+k:v for k,v in metrics.items()}\n",
        "        self.log_dict(metrics,on_epoch=True,prog_bar=True,logger=True)\n",
        "        return loss\n",
        "    \n",
        "    def setup(self, stage=None):\n",
        "        pass\n",
        "    \n",
        "    def prepare_data(self):\n",
        "        if self.df is not None:\n",
        "            complete_dataset = LSTMDataset(self.df,self.vocab,self.max_seq_length,self.pad_token,self.unk_token)\n",
        "            train_len,val_len = int(0.8*len(complete_dataset)),int(0.1*len(complete_dataset))\n",
        "            test_len = len(complete_dataset) - train_len - val_len\n",
        "            self.train_dataset,self.val_dataset,self.test_dataset = random_split(complete_dataset,[train_len,val_len,test_len])\n",
        "        else:\n",
        "            self.train_dataset = LSTMDataset(self.train_df,self.vocab,self.max_seq_length,self.pad_token,self.unk_token)\n",
        "            self.val_dataset = LSTMDataset(self.val_df,self.vocab,self.max_seq_length,self.pad_token,self.unk_token)\n",
        "            self.test_dataset = LSTMDataset(self.test_df,self.vocab,self.max_seq_length,self.pad_token,self.unk_token)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=int(self.batch_size))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=int(self.batch_size))\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=int(self.batch_size))\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        #define optimizers and LR schedulers\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "        #scheduler = CyclicLR(optimizer, base_lr = 5e-5,max_lr = 1e-3,step_size_up=100,step_size_down=200,cycle_momentum=False,verbose=False)\n",
        "        #return [optimizer],[{'scheduler': scheduler,'interval': 'step','frequency': 10}]#,'reduce_on_plateau': False,'monitor': 'val_loss',}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVQJNLxE8O5"
      },
      "source": [
        "Next we specify the configuration of our LightningModule.  \n",
        "\n",
        "Apart from model hyperparameters (eg- max_seq_length), we also need to pass:  \n",
        "1. configurations for initializing embeddings  \n",
        "  * if pre-trained embeddings used, provide:  \n",
        "    1. `pretrained_embeddings`: a numpy array of shape (size of vocabulary, embeddings dimension).\n",
        "    2. `freeze_embeddings`: `False` if embeddings to be trainable, `True` otherwise.\n",
        "    3. `vocab`: an iterable of shape (size of vocabulary,), each element a *token*.  \n",
        "    4. `pad_token`: token to be used as padding.\n",
        "    5. `unk_token`: token to be used for unknown tokens encountered in input sequences.  \n",
        "    **NOTE: `pad_token` and `unk_token` should already be present in `vocab`.**\n",
        "  * if pre-trained embeddings *NOT* used, provide:  \n",
        "    1. `vocab`: (same as above)\n",
        "    2. `embedding_dim`: desired embeddings dimension\n",
        "2. data\n",
        "  * if data is divided into train/val/test [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)s  \n",
        "    1. `train_df`\n",
        "    2. `val_df`\n",
        "    3. `test_df`\n",
        "  * if data is not split\n",
        "    1. `df`: this DataFrame will get randomly divided into train/val/test splits in an 80/10/10 ratio by default.  \n",
        "\n",
        "NOTE: all data DataFrames must contain 2 columns: `text` and `label`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZfJ3CQlSObM"
      },
      "source": [
        "config = {\n",
        "    #model configurations\n",
        "    'batch_size':32,\n",
        "    'max_seq_length':100,\n",
        "    'lr':1e-3,\n",
        "    'label_count':2,\n",
        "    'dropout_prob':2e-1,\n",
        "    'hidden_size':256,\n",
        "    'lstm_unit_cnt':2,\n",
        "\n",
        "    #embeddings configurations\n",
        "    'pretrained_embeddings':glove_embs_npa,\n",
        "    'freeze_embeddings':True,\n",
        "    'vocab':glove_terms_npa,\n",
        "    'pad_token':PAD_TOK,\n",
        "    'unk_token':UNK_TOK,\n",
        "\n",
        "    #data\n",
        "    'train_df':irony_train_df,\n",
        "    'val_df':irony_val_df,\n",
        "    'test_df':irony_test_df,\n",
        "}\n",
        "model = LitClassifier(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce4VW729NH2R"
      },
      "source": [
        "If your Jupyter environment has tensorboard support, feel free to execute the below cell to analyse the training and validation.  \n",
        "PyTorch Lightning logs to the directory `{current_directory}/lightning_logs/` by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zdb8Vaipuso"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"lightning_logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWb2-qqwNatQ"
      },
      "source": [
        "Finally, we initialize the PyTorch Lightning trainer and start the training process! ðŸŽ‰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmoC146un59l"
      },
      "source": [
        "trainer = pl.Trainer(max_epochs=10,gpus=1)\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236,
          "referenced_widgets": [
            "077fe79fc7de451c98685fd8e35f8f0a",
            "e9fd9549174844409c1d4e01bf6c97ad",
            "3a2667bfe3754275b65ebb6d0b1f3eb2",
            "fb41f9d75d6c4f8ea57368d61f6c83d7",
            "aec5b17d66ab4028987ddd411fffabed",
            "f2b68b2c2088497e8951731834d2dd21",
            "6c512db085124c36a4704f14deeb4fc1",
            "4bfb91e81fa9425580c2590ad964cac4"
          ]
        },
        "id": "A51j9CTuevc4",
        "outputId": "04890575-94cf-47e8-a81e-a5a8df5fae84"
      },
      "source": [
        "trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "077fe79fc7de451c98685fd8e35f8f0a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_acc': 0.5612244606018066,\n",
            " 'test_loss': 1.7929543256759644,\n",
            " 'test_macro_f1': 0.5529208183288574}\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_acc': 0.5612244606018066,\n",
              "  'test_loss': 1.7929543256759644,\n",
              "  'test_macro_f1': 0.5529208183288574}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZObZ7glfNMZ"
      },
      "source": [
        "ðŸ¤” Hmmm...the trained model performs well on the train set but quite poorly on the validation and test splits!\n",
        "\n",
        "Next step should be to explore regularization techniques like [EarlyStopping](https://pytorch-lightning.readthedocs.io/en/1.2.0/extensions/generated/pytorch_lightning.callbacks.EarlyStopping.html?highlight=earlystopping). Also, hyperparameter tuning can be a good idea.\n",
        "\n",
        "Good for us, these techniques are easily plugged into PyTorch Lightning!"
      ]
    }
  ]
}